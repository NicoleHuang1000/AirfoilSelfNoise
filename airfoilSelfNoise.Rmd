---
title: "AirfoilSelfNoise"
author: "Jiemin Huang"
output:
  pdf_document:
    df_print: paged
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
```

\newpage

## Assignment Questions

**Q1. (28 marks)**   The noise generated by an aircraft is an efficiency and environmental matter for the aerospace industry.  A vital component of the total airframe noise is the airfoil self-noise, caused by the interaction between an airfoil blade and the turbulence it produces. The airfoil self-noise dataset was obtained by NASA from a series of tests of airfoil blade sections conducted in an echoless wind-tunnel.  The dataset, obtained from the UCI Machine Learning Repository (https://archive.ics.uci.edu/dataset/291/airfoil+self+noise) and available in the file `airfoil_self_noise.csv`, contains the following variables:

*  `frequency`: Frequency, in Hertzs.
*  `angle`: Angle of attack, in degrees.
*  `chord_length`:  Chord length, in metres.
*  `speed`: Free-stream velocity, in metres per second.
*  `displace`: Suction side displacement thickness, in meters.
*  `decibels`: Scaled sound pressure level, in decibels. 

We will carry out a regression analysis to investigate the relationship between the response variable `decibels` and the other variables in the dataset as predictors.  

a.   **(4 marks)** Carry out an exploratory data analysis (EDA). NOTE: The predictors `chord.length` and `speed` are numerical, but only have a few different values each (6 for `chord.length` and 4 for `speed`).  Such variables are best treated as categorical variables during the analysis.  List any key points of note from your EDA, including any considerations you might make during a regression analysis.

```{r }
library(dplyr)
noise <- read.csv("airfoil_self_noise.csv")
str(noise)
noise$chord.length <- as.factor(noise$chord.length)
noise$speed <- as.factor(noise$speed)

```
```{r}
summary(noise)
```
```{r}
#scatterplot matrix for numerical variables
library(dplyr)
library(psych)

noise%>%
  dplyr::select(where(is.numeric))%>%
  pairs.panels(method = "spearman",
               hist.col = "lightgreen",
               density = TRUE,
               ellipses = FALSE)
```
```{r}
#box plot of noise for categorical predictors
library(ggplot2)

noise%>%
  dplyr::select(decibels, where(is.factor))%>%
  ggplot(aes(x = chord.length, y = decibels)) + geom_boxplot()
```
```{r}
noise%>%
  dplyr::select(decibels, where(is.factor))%>%
  ggplot(aes(x = speed, y = decibels)) + geom_boxplot()
```

Comments on the visualizations :

(1): The median decibels for chord length which are within (0.0254, 0.1016) are higher than those for chord length are within (0.1524, 0.3048). 

(2): The median decibels for the three low levels of chord length are almost the same.And the median decibels for the three high levels of chord length are almost the same.

(3): The median decibels for the two higher speed are slighter higher than those for two lower levels.

(4): Decibels stay constant when frequency is low, then it decreases with higher frequency, but the decrease tails off after 5000 frequency.

(5): Decibels decreases while angle increase, while it has a fluctuate when angle become around 6, reaching highest decibel.

(6): Decibels decreases while displace value increase, while it has a fluctuation when displace value increase to approximate 0.005, reaching lowest decibel.

(7): The distribution of the response variable decibels is not symmetrical - a transformation or other alternative for dealing with non-normality may be required if indicated by residual diagnostics.

(8): There is potential non-linearity in the relationship between decibels and each of the numerical predictors.Polynomial or smooth spline regression should be considered.

(9): Based on the spearman correlation coefficients, the comparetively strongest predictors of decibels are frequency and displace.A variable selection procedure should be implemented.

(10): There are moderate correlations between some pairs of predictors - multicollinearity is not likely to be an issue, but should be investigated.

b.  **(3 marks)**  Fit a linear model to the data, including all predictors with no transformations or interactions.  Present a summary of the model in a table and write the fitted model equation.  Give an estimate of $\sigma^2$, the error variance.

```{r}
fit1 <- lm(decibels ~ frequency + angle + chord.length + speed + displace, data = noise)
library(pander)
pander(summary(fit1), caption = "")
```
the fitted model equation is:
     $\hat{noise}\ = 135.8 - 0.001295frequency - 0.47angle - 1.603chord.length0.0508 - 2.951chord.length0.1016 -6.551chord.length0.1524 - 7.71chord.length0.2286 - 10.22chord.length0.3048 + 0.7997speed39.6 + 2.252speed55.5 + 4.077speed71.3 - 125.7displace$ 

the error variance is:
     $\sigma^2 = 4.773 ^2 = 22.78$

c.  **(3 marks)**  Based on the fitted model results in part (b), give an interpretation of the coefficients for `angle` and `speed71.3`. 

(1) The coefficient for angle is  -0.47. It represents the change in expected decibels associates with a 1-unit increase in angle when all other predictors are held constant. In particular, we can say that when angle increase by 1 degree there is an associated reduction in expected decibel of 0.47 decibels,when all other predictors are kept constant.

(2) The coefficient for speed71.3 is 4.077. It indicates that speed31.7 is used as a reference level and 4.077 is the estimated difference $E[Y|X_i = "71.3"]-E[Y|X_i = "31.7"]$ (i=10), keeping all other predictors constant. It means the expected decibel when speed value is 71.3 is higher than the expected decibel when speed value is 31.7 by an estimated 4.077 decibels when comparing noises of equal frequency, angle,chord.length and displace.

d.  **(2 marks)**  Does it make practical sense to interpret the intercept in this case?  Justify your answer.

Since the predictor values of zero does not make sense for real situations, I think we don't need to interpret the intercept.

e.  **(3 marks)** Obtain 95% confidence and prediction intervals for the last three observations in the dataset.  Explain briefly why the prediction intervals are wider than the confidence intervals.

```{r}
xdata <- subset(tail(noise, 3))
pander(predict(fit1, newdata = xdata, interval = "confidence"),
       caption = "confidence intervals", round = 2)
pander(predict(fit1, newdata = xdata, interval = "prediction"),
       caption = "prediction intervals")
```

The prediction intervals try to capture the variation of all points around the fitted line as well as uncertainty about where the line truly lies. While confidence intervals only capture uncertainty about the line.That's why prediction intervals are wider than confidence intervals.

f.  **(4 marks)**  Use the `plot` function to carry out residual diagnostics for the model you fitted in part (b).  Comment on what the residual plots indicate about regression assumptions or the existence of influential observations.

```{r}
par(mfrow=c(1,2))
plot(fit1)
```

Comments on regression assumptions or the existence of influential observations as follows:

(1) Residuals vs fitted plot:there is a curved pattern showed on the graph, which means there are potential non-linear relationships between predictors and response variable. Predictor transformations may be required.

(2)Q-Q plot:Several deviation from a straight line is evidence of potential non-normality.Transformation of response variable may help.

(3)Scale-location plot:A change in the spread of points as fitted values change is evidence of potential non-constant variance.Transformation may help.

(4)Residual vs leverage plot:all cases are well inside of the Cook's distance thresholds, which means there are no influential observations.However, two observations 1165 and 1030, are labelled as being potentially problematic in all four plots. These observations have large residuals, but do not have undue influence on the regression model. It is worth looking at the noise corresponding to these observation numbers to see if there is anything special about them.

g.  **(4 marks)**  Test the assumptions of normality and constant variance in the errors.  Do the results confirm the conclusions you reached in part (f) about these assumptions? In your response, include the hypotheses being tested in each test.

```{r}
shapiro.test(fit1$residuals)
ks.test(fit1$residuals, "pnorm")
library(lmtest)
bptest(fit1)
```

(1)Normality assumption: Shapiro-Wilkâ€™s W test and the Kolmogorov-Smirnov (K-S) test

  $H_0$: The sample comes from a normal distribution
  
  $H_1$: The sample does not come from a normal distribution
  

(2)Equal variance assumption:  Breush-Pagan test

  $H_0$: Homoscedasticity is present (the residuals are distributed with equal variance)
  
  $H_1$: Heteroscedasticity is present (the residuals are not distributed with equal variance)

For Shapiro-Wilks test, we have w=0.99445,, p-value<0.05.

For Kolmogorov-Smirnov test, we have D = 0.30779, p-value < 0.05.

For Breush-Pagan test, we have BP=177.87, df=11, p-value<0.05.

We reject the null hypothesis in the Shapiro-Wilks and Kolmogorov-Smirnov tests, indicating there is no evidence that residuals come from a normal distribution. It confirms the non-normality conclusion from (f).

We reject the null hypothesis in Breush-Pagan test suggesting there is insufficient evidence that homoscedasticity is present. This indicates that the non-constant variance we suspect from the diagnostic plots is substantial.It confirms the non-constant variance conclusion from (f).

h.  **(2 marks)** Use the VIF statistic to check whether or not there is evidence of severe multicollinearity among the predictors.  Comment on the results.

```{r}
library(car)
library(knitr)
library(pander)
pander(vif(fit1), digits = 2, caption = "VIF values")
```

(1)Interpretation: There is no evidence of severe multicollinearity, since all VIF values are less than 10.

(2)The largest VIF value tells us that the variance of the angle coefficient is inflated by a factor of 4.6 because angle is highly correlated with at least one of the other predictors in the model.

i.  **(3 marks)** Based on a global usefulness test, is it worth going on to further analyse and interpret a model of `decibels` against each of the predictors?  Carry out the test, give the conclusion and justify your answer.

```{r}
summary(fit1)
```

(1) Global usefulness test:

  $H_0$: $\beta_1$ =$\beta_2$ =...=$\beta_p$ = $0$
  
  $H_1$: at least one $\beta_j$ is non-zero, for $j = 1, \ldots, p$

  
(2) In this case, we find F = 149.7 with 11 and 1491 degrees of freedom and p-value < $2 \times 10^{-16}$ ,so p-value <0.05,We therefore have very strong evidence to reject $H_0$ in this case and conclude that there is no evidence that all regression coefficients are zero.So we can go further analyse and interpret the model of decibels against the predictors.

\newpage

**Q2. (12 marks)**  Francis Galton's 1866 dataset (cleaned) lists individual observations on height for 899 children.  Galton coined the term "regression" following his study of how children's heights related to heights of their parents. The data are available in the file `galton.csv` and contain the following variables:

*  `familyID`:  Family ID
*  `father`: Height of father
*  `mother`: Height of mother
*  `gender`: gender of child
*  `height`: Height of child
*  `kids`:  Number of childre in family
*  `midparent`: Mid-parent height calculated as (`father + 1.08*mother)/2
*  `adltchld`: `height` if `gender`=M, otherwise 1.08*`height` if `gender`= F 

All heights are measured in inches.

a. **(3 marks)**  Read the data into R and fit a linear model for `height` with the variables `father`, `mother`, `gender`, `kids` and `midparent` as predictors.  Provide a summary of the fitted model.  You will notice that estimates for `midparent` are listed as `NA`.  Why might this be the case and what regression problem does this point to?

```{r}
library(pander)
galton<-read.csv("galton.csv", header=T) 
galton$gender <- as.factor(galton$gender)
str(galton)
fit1 <- lm(height ~ father + mother + gender + kids + midparent, data = galton)
pander(summary(fit1), caption = "")
```
```{r}
summary(galton)
```
There is a linear relationship between 'mid-parent', 'father' and 'mother', so there is an perfect multicollinearity issue. So the regression cannot uniquely determine the individual effects of 'father' and 'mother' on the response variable because there are infinitely many combinations of coefficients for A and B that perfectly predict C.That may be the problem why estimates for 'mid-parent' is missing.

b. **(2 marks)**  What action might you take to resolve the problem identified in part (a)?

To address the issue, we can exclude predictor 'mid-parent' from the model or rethinking the set of predictors.

c. **(2 marks)**  Based on the model fitted in part (a) give an interpretation of the coefficient for `genderM`.

We treated 'gender' as a categorical variable, and the table gives an estimate for gender='M'. Gender = 'F' is used as a reference level.
Coefficient for 'genderM' is $\beta_3$$=5.21$. It is the estimated difference $E[Y|X_3='M']-E[Y|X_3='F']$, keeping all the other predictors constant. It means the expected height for boys is higher than the expected height for girls by an estimated 5.21 inches when comparing height of equal 'father','mother' and 'kids'.

d. **(2 marks)**   Determine the number of families in the dataset.

```{r}
num_families <- length(unique(galton$familyID))
num_families
```
There are 197 families in the dataset.

e. **(3 marks)**  The problem in part (a) is resolved and a new linear model is fitted.No observations are excluded.  The plots below are obtained to investigate regression assumptions for this new model.  Based on your answer in part (d) and the plots below, do the data meet all the regression assumptions?  Explain your answer briefly.

```{r, fig.align='center', echo=FALSE, out.width='90%'}
galton<-read.csv("galton.csv", header=T)
fit2<-lm(height ~ father  +  kids + gender + midparent, data=galton)
par(mfrow=c(2,2))
plot(fit2)

```

(1) Residuals vs fitted plot:The residuals are equally spread around a horizontal line without a distinct pattern, indicating there are no uncaptured significant non-linear relationship.

(2)Q-Q plot:No severe deviation from a straight line, indicating normality assumption is met.

(3)Scale-location plot:The spread of points do not change as fitted values change, indicating non-constant variance assumption is met.

(4)Residual vs leverage plot:Cookâ€™s distance lines are invisible because all cases are well inside of the Cookâ€™s distance thresholds, thus there are no highly influential observations.

Thus, I think the data meet all the regression assumptions.
